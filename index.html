<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Dr Safa Mefteh, Ph.D., Eng. ‚Äì AI & Computer Vision Researcher & Developer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!-- <meta name="description" content="Portfolio of Dr Safa Mefteh, AI & Computer Vision Researcher & Developer." /> -->
  <meta name="description" content="Dr. Safa Mefteh ‚Äì AI & Computer Vision Researcher & Developer based in Belgium. Specialized in object detection, semi-supervised learning, explainability, and embedded AI.">
  <meta name="keywords" content="Safa Mefteh, AI Researcher, Computer Vision, Machine Learning, Belgium, Object Detection, Semi-Supervised Learning, NVIDIA Jetson">
  <meta name="author" content="Safa Mefteh">

  <!-- Google Font (optional) -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css?v=20251217-1">
  
  <link rel="icon" type="image/png" href="imgs/safa_71x64.png">

</head>

<script>
document.addEventListener("DOMContentLoaded", () => {
  const items = Array.from(document.querySelectorAll(".pill-item"));

  function getTextBlock(item) {
    // Experience uses .exp-text, publications use .pub-abstract
    return item.querySelector(".exp-text") || item.querySelector(".pub-abstract");
  }

  function needsToggle(el) {
    // if no clamp CSS, this will always be false
    el.setAttribute("data-collapsed", "false");
    const full = el.scrollHeight;

    el.setAttribute("data-collapsed", "true");
    const clamped = el.getBoundingClientRect().height;

    return full > clamped + 2;
  }

  items.forEach(item => {
    const text = getTextBlock(item);
    const btn  = item.querySelector(".exp-toggle");
    if (!text || !btn) return;

    // Ensure initial state
    if (!text.getAttribute("data-collapsed")) {
      text.setAttribute("data-collapsed", "true");
    }

    // Hide button if not needed
    if (!needsToggle(text)) {
      btn.classList.add("is-hidden");
      text.setAttribute("data-collapsed", "false");
      return;
    }

    btn.addEventListener("click", () => {
      const collapsed = text.getAttribute("data-collapsed") === "true";

      // Optional: close others
      items.forEach(other => {
        if (other === item) return;
        const ot = getTextBlock(other);
        const ob = other.querySelector(".exp-toggle");
        if (ot && ob && !ob.classList.contains("is-hidden")) {
          ot.setAttribute("data-collapsed", "true");
          ob.textContent = "Read more";
        }
      });

      text.setAttribute("data-collapsed", collapsed ? "false" : "true");
      btn.textContent = collapsed ? "Read less" : "Read more";
    });
  });
});
</script>



<body>
<div class="page">
  <!-- NAVBAR -->
  <header class="navbar">
    <div class="nav-inner">
      <div class="nav-brand">
        <div class="nav-avatar">S</div>
        <div>
          <div class="nav-title-main">Dr Safa Mefteh, Ph.D., Eng.</div>
          <div class="nav-title-sub">AI & Computer Vision Researcher & Developer</div>
        </div>
      </div>
      <nav class="nav-links">
        <a href="#about">About</a>
        <a href="#skills">Skills</a>
        <a href="#experience">Experience</a>
        <a href="#publications">Publications</a>
        <a href="#contact">Contact</a>
      </nav>
    </div>
  </header>

  <!-- HERO -->
  <main>
    <section class="hero">
      <div class="hero-left">
        <div class="hero-tag">
          <span class="hero-tag-dot"></span>
          <span>Applied AI ¬∑ Computer Vision ¬∑ Edge & Robotics</span>
        </div>

        <h1 class="hero-title">
          I'm Safa Mefteh, Ph.D.
        </h1>

        <p class="hero-subtitle">
          I‚Äôm an <strong>AI &amp; Computer Vision researcher</strong> with a passion for building 
          <strong>intelligent perception systems</strong> that operate reliably in real-world environments. 
          My work spans <strong>object detection</strong>, <strong>multimodal learning</strong>, 
          <strong>model explainability</strong>, and <strong>embedded deployment</strong>, 
          with a strong focus on translating advanced deep learning algorithms into 
          <strong>robust, real-time solutions on edge devices</strong>. 
          I thrive at the intersection of <strong>research and engineering</strong>, 
          where performance, efficiency, and interpretability must coexist.
        </p>
        <div class="hero-buttons">
          <a href="#experience" class="btn-primary">View my experience</a>
          <a href="#publications" class="btn-outline">Scientific publications</a>
        </div>

        <div class="hero-meta">
          <div class="hero-meta-item">
            <span class="hero-meta-dot"></span>
            <span>AI, Computer Vision & Machine Learning</span>
          </div>
          <div class="hero-meta-item">
            <span class="hero-meta-dot" style="background:#27ae60;"></span>
            <span>Object detection ¬∑ SSOD ¬∑ Explainability</span>
          </div>
          <div class="hero-meta-item">
            <span class="hero-meta-dot" style="background:#f2994a;"></span>
            <span>Edge AI ¬∑ Robotics ¬∑ Smart Video Analysis</span>
          </div>
        </div>
      </div>

      <div class="hero-right">
        <div class="profile-card">
          <div class="profile-header">
            <!-- <div class="profile-avatar" style="
                width:90px;
                height:90px;
                border-radius:50%;
                overflow:hidden;
                box-shadow:0 8px 18px rgba(47,128,237,0.25);">
              <img src="safa2.png" alt="Safa Mefteh" style="width:100%; height:100%; border-radius:50%; object-fit:cover;">
            </div> -->
            <div class="profile-avatar">
              <img src="imgs/safa.png" alt="Safa Mefteh">
            </div>
            <div>
              <div class="profile-name">Dr Safa Mefteh, Ph.D., Eng.</div>
              <div class="profile-role">AI & Computer Vision Researcher & Developer</div>
              <div class="profile-pill">
                Currently ¬∑ Computer Vision & ML researcher & developer, Multitel
              </div>
            </div>
          </div>

          <div class="profile-tags">
            <span class="chip">Deep Learning</span>
            <span class="chip">Computer Vision</span>
            <span class="chip">Edge AI</span>
            <span class="chip">Research & Development (R&D)</span>
            <span class="chip">C++ ¬∑ Python ¬∑ PyTorch ¬∑ OpenCV</span>
          </div>

          <div class="profile-footer">
            <span>üéì <strong>Ph.D.</strong><br/>Computer Vision & Deep Learning</span>
            <span>üõ† <strong>Industry</strong><br/>Robotics & Intelligent Video Analytics</span>
          </div>
        </div>
      </div>
    </section>

    <!-- ABOUT & EDUCATION -->
    <section id="about">
      <div class="section-heading">
        <div>
          <div class="section-kicker">Profile</div>
          <h2 class="section-title">About & Education</h2>
        </div>
      </div>

      <div class="section-grid">
        <div class="card">
          <div class="card-title">About me</div>
          <p>
            I work at the intersection of <strong>advanced algorithms</strong> (computer vision, deep learning, multimodal learning, explainability) 
            and <strong>real-world systems</strong> where latency, reliability, and hardware efficiency are critical. 
            I design, train, and deploy <strong>high-performance perception models</strong> with a strong focus on 
            <strong>Edge AI optimization</strong>, transforming complex visual data into robust, real-time intelligence on embedded platforms.
          </p>

          <div class="badge-row" style="margin-top:10px;">
            <span class="badge">Computer Vision & Deep Learning</span>
            <span class="badge">Edge AI ¬∑ NVIDIA Jetson</span>
            <span class="badge">Explainability ¬∑ Multimodal ML</span>
            <span class="badge">Efficient model deployment</span>
            <span class="badge">Research & Engineering synergy</span>
          </div>
        </div>

        <div class="card">
          <div class="card-title">Education</div>
          <div class="timeline">
            <div class="timeline-item">
              <div class="timeline-marker"></div>
              <div>
                <div class="timeline-title">Ph.D. in Computer Vision & Deep Learning</div>
                <div class="timeline-meta">Higher School of Communication of Tunis (Sup‚ÄôCom), University of Carthage </div>
              </div>
            </div>
            <div class="timeline-item">
              <div class="timeline-marker"></div>
              <div>
                <div class="timeline-title">Engineer‚Äôs Degree in Computer Science</div>
                <div class="timeline-meta">National School of Computer Science, Tunisia </div>
              </div>
            </div>
            <div class="timeline-item">
              <div class="timeline-marker"></div>
              <div>
                <div class="timeline-title">Preparatory Diploma in Mathematics & Physics</div>
                <div class="timeline-meta">Preparatory Institute for Studies of Engineers of Monastir </div>
              </div>
            </div>
            <div class="timeline-item">
              <div class="timeline-marker"></div>
              <div>
                <div class="timeline-title">Bachelor‚Äôs Degree in Mathematics</div>
                <div class="timeline-meta">Hammam Sousse 2 High School, Tunisia </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- SKILLS -->
    <section id="skills">
      <div class="section-heading">
        <div>
          <div class="section-kicker">What I work with</div>
          <h2 class="section-title">Skills</h2>
        </div>
        <div class="section-note">
          A mix of research skills, engineering practices and deployment experience.
        </div>
      </div>

      <div class="section-grid">
        <!-- <div class="card"> -->
          <div class="card">
            <div class="card-title">Technical skills</div>

            <div class="badge-row">
              <span class="badge">Computer Vision</span>
              <span class="badge">Deep Learning</span>
              <span class="badge">Multimodal Perception</span>
              <span class="badge">Semi-Supervised Object Detection</span>
              <span class="badge">Model Explainability </span>
              <span class="badge">Detection ¬∑ Segmentation ¬∑ Tracking</span>
              <span class="badge">Pseudo-Labeling & Multi-View Learning</span>
              <span class="badge">Feature Engineering</span>
            </div>

            <div class="badge-row" style="margin-top:10px;">
              <span class="badge">Python ¬∑ C++</span>
              <span class="badge">PyTorch</span>
              <span class="badge">TensorRT</span>
              <span class="badge">DeepStream</span>
              <span class="badge">NVIDIA Jetson (Nano ¬∑ Xavier ¬∑ Orin)</span>
              <span class="badge">GPU Optimization</span>
              <span class="badge">Docker ¬∑ Git</span>
              <span class="badge">ROS</span>
            </div>
          </div>

          <div class="card">
            <div class="card-title">Soft skills & languages</div>
            <p>
              I appreciate clear teamwork, structured projects, and a collaborative way of working.
            </p>

            <div class="badge-row" style="margin-top:10px;">
              <span class="badge">Team leadership</span>
              <span class="badge">Mentoring & knowledge sharing</span>
              <span class="badge">Project ownership</span>
              <span class="badge">Cross-functional collaboration</span>
            </div>

            <div class="badge-row" style="margin-top:14px;">
              <span class="badge">French ¬∑ Fluent</span>
              <span class="badge">Arabic ¬∑ Native</span>
              <span class="badge">English ¬∑ Professional working proficiency</span>
            </div>
          <!-- </div> -->
    </section>

    <!-- EXPERIENCE -->
    <section id="experience">
      <div class="section-heading">
        <div>
          <div class="section-kicker">Career path</div>
          <h2 class="section-title">Experience</h2>
        </div>
      </div>

      <div class="section-grid">
        <div class="card">
          <div class="card-header-left">
            <img src="imgs/multitel.png" alt="Multitel Logo" class="company-logo">
            <div>
              <div class="card-title">Computer Vision &amp; ML Researcher &amp; Developer</div>
              <div class="card-subtitle">Multitel ¬∑ Jun 2023 ‚Äì Present ¬∑ Mons, Belgium</div>
            </div>
          </div>

          <div class="pill-list">
            <!-- Project 1: SSOD -->
            <div class="pill-item">
              <strong>Advanced Semi-Supervised Object Detection (SSOD) for Real-Time Edge Deployment</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                <p>
                  Large-scale, high-quality annotations are often <strong>scarce</strong> and <strong>costly</strong>. 
                  Semi-supervised learning leverages abundant <strong>unlabeled data</strong> to preserve strong detection performance 
                  while reducing annotation dependency. A key challenge is minimizing the <strong>distribution gap</strong> between 
                  labeled and unlabeled data‚Äîespecially across diverse and dynamic contexts. Another critical issue is generating 
                  <strong>reliable pseudo-labels</strong>, since early errors can propagate and degrade overall performance.
                </p>

                <p>
                  To address these challenges, we designed and deployed a <strong>real-time SSOD framework</strong> for one-stage 
                  RGB video analysis on <strong>edge devices</strong>. Our approach integrates foundation models 
                  (<strong>Grounding-DINO</strong> and <strong>YOLO-World</strong>) to improve object coverage and semantic robustness, 
                  achieving a <strong>+3 mAP</strong> improvement over state-of-the-art baselines. In addition, we introduced a 
                  redesigned pseudo-labeling strategy combining <strong>multi-view consistency</strong> with 
                  <strong>fusion-based optimization</strong>, reducing label noise and narrowing the labeled‚Äìunlabeled domain gap, 
                  yielding a further <strong>+4 mAP</strong> gain.
                </p>

                <p>
                  The framework was validated across <strong>multiple domains</strong> and optimized for <strong>real-time inference</strong> 
                  on <strong>NVIDIA Jetson</strong> platforms, demonstrating both <strong>robustness</strong> and 
                  <strong>deployment-level efficiency</strong>.
                </p>
                  
              </div>
                
              </div>
              <button class="exp-toggle" type="button">Read more</button>
            </div>
            <!-- Project 2: Interpretation Toolbox -->
            <div class="pill-item">
              <strong>Interpretation Toolbox for Object Detection</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                  <p>
                    <strong>Mean Average Precision (mAP)</strong> is the standard metric for evaluating object detection models, yet it provides only
                    a global performance score and offers limited insight into <strong>why</strong> a model succeeds or fails. A degradation in mAP may
                    originate from diverse error sources, such as <strong>misclassification</strong>, <strong>inaccurate localization</strong>,
                    <strong>missed detections</strong>, <strong>background false positives</strong>, or <strong>duplicated predictions</strong>.
                  </p>
                  Moreover, factors like <strong>object scale</strong>, <strong>distance to the camera</strong>, and <strong>visual conditions</strong>
                  can significantly affect detection quality, making a deeper and more structured analysis essential.
                <!-- </p> -->

                <p>
                  To address these limitations, we developed an <strong>interpretation toolbox</strong> for object detection that enables a fine-grained,
                  diagnostic evaluation of model behavior. The toolbox automatically categorizes detection errors‚Äîincluding
                  <strong>classification errors</strong>, <strong>localization errors</strong>, <strong>combined classification‚Äìlocalization errors</strong>,
                  <strong>missed objects</strong>, <strong>background false positives</strong>, and <strong>duplicate detections</strong>‚Äîand explicitly links
                  each error type to its contribution to <strong>mAP degradation</strong>. This allows practitioners to move beyond a single aggregate metric
                  and systematically identify the dominant sources of performance loss.
                </p>

                <p>
                  The toolbox decomposes mAP into interpretable error components and
                  quantifies the upper-bound performance achievable under perfect error correction. Detection results are visualized through an
                  <strong>interactive interface</strong> where objects are grouped by error type, enabling rapid identification of systematic failure modes
                  (e.g., poor performance on <strong>small</strong> or <strong>distant</strong> objects). To further uncover patterns across errors, detected
                  objects are clustered by applying a visual feature extraction, followed by a dimensionality
                  reduction for intuitive 2D visualization of error clusters.
                </p>

                <p>
                  In addition, the toolbox integrates <strong>pixel-level</strong>, perturbation-based explainability by generating
                  <strong>saliency maps</strong> that highlight the image regions most influential for each detection. This enables inspection of the model‚Äôs
                  internal decision process, revealing whether critical semantic cues‚Äîsuch as <strong>wheels</strong> in daylight scenes or
                  <strong>headlights</strong> at night for vehicle detection‚Äîare effectively leveraged. When such cues are absent or ignored, the toolbox helps
                  explain persistent misdetections or misclassifications.
                </p>

                <p>
                  Overall, this interpretation toolbox provides a comprehensive <strong>visual</strong> and <strong>quantitative</strong> analysis framework for
                  object detection models, improving <strong>trust</strong>, <strong>transparency</strong>, and <strong>diagnostic accuracy</strong>. It supports
                  informed model debugging, targeted data augmentation, and architecture refinement, making it particularly valuable for evaluating and
                  deploying object detection systems in <strong>real-world</strong> and <strong>safety-critical</strong> applications.
                </p>
              </div>
              </div>
              <button class="exp-toggle" type="button">Read more</button>
              <!-- <a class="pill-link" href="interpretation-toolbox.html">
                <span class="pill-icon">üìñ</span>
                Read more
              </a> -->
            </div>
            <!-- Project 3: VMS Integration -->
            <div class="pill-item">
              <strong>Edge-to-VMS Integration for Real-Time Video Analytics</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                Integrated embedded detection models into a Video Management System (VMS) by connecting
                <strong>NVIDIA Jetson Orin</strong> inference pipelines to centralized video servers.
                Implemented custom bridging functions inspired by DeepStream SDK VPS‚ÄìMilestone workflows,
                enabling event streaming, metadata forwarding, and simulation of licensed VMS analytics features
                for seamless operational deployment.
                </div>
              </div>
              <button class="exp-toggle" type="button">Read more</button>
            </div>

          </div>
        </div>

        <div class="card">
          <div class="card-header card-header--with-project">
            <div class="card-header card-header-left">
              <img src="imgs/enova.png" alt="Enova Robotics Logo" class="company-logo">
              <div>
                <div class="card-title">
                  Senior ML &amp; Computer Vision R&amp;D Engineer ¬∑ AI Team Lead (P-Guard Robot)
                </div>
                <div class="card-subtitle">Enova Robotics ¬∑ Jan 2019 ‚Äì May 2023 ¬∑ Tunisia</div>
              </div>
            </div>

            <img src="imgs/pguard.jpeg" alt="P-Guard Robot" class="project-logo">
          </div>

          <div class="pill-list">

            <div class="pill-item">

              <strong>Intelligent Video Analysis (Day &amp; Night) ‚Äì P-Guard Security Robot</strong>

              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                  <p>
                    I designed and deployed a robust <strong>AI-based intrusion detection system</strong> for the
                    <strong>P-Guard autonomous security robot</strong>, leveraging <strong>four heterogeneous cameras</strong>
                    (three optical and one thermal) to ensure reliable people detection and fence-intrusion monitoring under both
                    <strong>daytime and nighttime</strong> conditions. The system was engineered for dynamic outdoor environments,
                    addressing critical challenges such as <strong>robot and target motion</strong>, <strong>variable illumination</strong>,
                    <strong>adverse weather</strong> (rain, fog, humidity), <strong>occlusions</strong>, and
                    <strong>diverse human postures</strong> (standing, crouching, lying, walking, running), while maintaining
                    <strong>low false-positive rates</strong> and <strong>real-time alarm triggering</strong>.
                  </p>

                  <p>
                    To meet these constraints, I built a <strong>custom multi-modal dataset</strong> collected directly from the robot,
                    capturing people at different distances, viewpoints, and partial visibilities across optical and thermal sensors.
                    The dataset was manually annotated using <strong>LabelImg</strong> and used to fine-tune state-of-the-art detectors
                    (including <strong>YOLO variants</strong> and <strong>Faster R-CNN</strong>) via transfer learning. After extensive
                    benchmarking and on-robot inference tests, I selected <strong>YOLOv5s</strong> for its strong balance of accuracy
                    and computational efficiency.
                  </p>

                  <p>
                    The final solution was deployed on an <strong>NVIDIA Jetson Nano</strong> using the <strong>DeepStream SDK</strong>,
                    enabling simultaneous real-time inference on all four streams. The model was optimized and exported as a
                    <strong>TensorRT engine</strong> to minimize latency and maximize throughput. Detection metadata was transmitted via
                    the <strong>Milestone Video Processing Service (VPS) Toolkit</strong> and integrated into
                    <strong>Milestone XProtect VMS</strong>, where it was used to draw bounding boxes, trigger alarms, and generate
                    events for end users‚Äîvalidating the system‚Äôs reliability and deployment readiness for real-world security scenarios.
                  </p>
                </div>
              </div>
              <button class="exp-toggle" type="button">Read more</button>
            </div>

            <div class="pill-item">
              <strong>Autonomous Charging Station</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                  <p>
                    I developed and deployed a <strong>traditional computer-vision</strong> based autonomous charging solution to reliably connect the robot to its
                    docking station and maximize operational autonomy. The system detects the robot‚Äôs charging contacts (‚Äúcosses‚Äù) using
                    <strong>OpenCV</strong> by identifying a simple <strong>green-light cue</strong>, then tracks this cue with the
                    <strong>Lucas‚ÄìKanade optical flow</strong> method to robustly localize its center once the robot is positioned in
                    front of the station.
                  </p>
                  <p>
                    Based on the estimated target center, I designed a <strong>control law</strong> to drive the station‚Äôs
                    <strong>X‚ÄìY table</strong> and precisely align the connector with the robot‚Äôs contacts. Once alignment is achieved,
                    the system activates a <strong>relay</strong> to safely enable charging current, delivering an end-to-end closed-loop
                    docking and charging workflow.
                  </p>
                </div>
              </div>
              <button class="exp-toggle" type="button">Read more</button>
            </div>

            <div class="pill-item">
              <strong>Obstacle Detection, Distance Estimation, and Autonomous Avoidance</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                 <p>
                  The <strong>P-Guard robot</strong> operates as a <strong>fully autonomous outdoor security platform</strong>,
                  deployed across sites with heterogeneous characteristics such as industrial areas, logistics zones, and railway
                  environments. In these dynamic and unstructured settings, the robot must patrol and perform surveillance while
                  interacting safely and naturally with <strong>moving obstacles</strong> including <strong>trucks, trains, and
                  vehicles</strong>, without disrupting its mission continuity.
                </p>

                <p>
                  To address this challenge, I developed an <strong>obstacle detection and decision-making framework</strong>
                  combining vision and LiDAR sensing. A dedicated dataset of truck images was collected and used to train a
                  <strong>YOLOv7-based object detector</strong> for robust vehicle recognition in outdoor scenes. Once detected, the
                  <strong>distance between the robot and the obstacle</strong> is estimated using a hybrid approach that leverages
                  both the <strong>area of the detected bounding box</strong> and depth information provided by a
                  <strong>Velodyne VLP-16 LiDAR</strong>, enabling reliable distance estimation at ranges exceeding
                  <strong>5 meters</strong>.
                </p>

                <p>
                  Based on the estimated distance and object dynamics, the robot autonomously adapts its behavior. When a detected
                  vehicle lies within the robot‚Äôs field of view, a <strong>stopped vehicle</strong> is treated as a static obstacle
                  and safely bypassed while patrol continues. Conversely, when a vehicle is in motion, the robot
                  <strong>halts and waits</strong> until the path is clear before resuming its patrol. The proposed solution was
                  validated through real-world outdoor trials, including <strong>train environments</strong>, demonstrating robust
                  perception, safe decision-making, and seamless integration into the robot‚Äôs autonomous surveillance workflow.
                </p> 
                </div>
              </div>
              <button class="exp-toggle" type="button">Read more</button>
            </div>

            <div class="pill-item">
              <strong>Fence Anomaly Detection</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                  <p>
                    I led a <strong>proof-of-concept study</strong> on <strong>fence anomaly detection</strong>, a problem that remains
                    largely unexplored in the literature, where existing state-of-the-art works mainly address
                    <strong>de-fencing</strong> rather than the detection of <strong>broken or damaged fences</strong>. The objective of
                    this work was to evaluate the feasibility of automatically assessing fence integrity in real-world security
                    scenarios.
                  </p>

                  <p>
                    To support this study, we curated a dataset composed of images <strong>captured directly by the robot</strong> across
                    diverse outdoor environments, complemented with additional broken-fence images collected from public sources. A
                    <strong>Pix2Pix GAN</strong> was trained to generate <strong>fence masks</strong> from real images, enabling robust
                    isolation of fence structures under varying visual conditions. The resulting masked images were further processed
                    using <strong>adaptive Gaussian thresholding</strong> to emphasize structural discontinuities associated with fence
                    damage.
                  </p>

                  <p>
                    On top of this representation, we developed a <strong>ResNet-18‚Äìbased classification model</strong> to distinguish
                    between <strong>intact</strong> and <strong>broken fences</strong>. While the approach showed promising results during
                    daytime, <strong>night-time detection using thermal cameras proved technically impractical</strong>, as fences would
                    need to emit or retain sufficient heat to be visible‚Äîan assumption that is rarely realistic in operational
                    conditions üòè. This limitation highlights the constraints of thermal sensing for static structures and motivates
                    alternative sensing or illumination strategies for future work.
                  </p>
                </div>
              </div>
              <button class="exp-toggle" type="button">Read more</button>
            </div>

            <div class="pill-item">
              <strong>Project Management</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                  <ul>
                    <li>
                      <strong>Technical Leadership &amp; Reporting:</strong>
                      Managed and coordinated <strong>cross-disciplinary teams</strong> including
                      <strong>front-end and back-end developers</strong>, as well as <strong>robotics and AI engineers</strong>,
                      ensuring effective collaboration across software, perception, and embedded systems.
                      Led AI-focused workstreams and delivered <strong>weekly progress reports</strong> on AI advancements and milestones
                      to the <strong>CTO</strong>, ensuring alignment, transparency, and timely delivery.
                    </li>

                    <li>
                      <strong>P-Guard Mission Scheduling:</strong>
                      Led the design and implementation of a fully autonomous mission-scheduling framework, enabling P-Guard to execute
                      patrol missions and operational scenarios <strong>without human intervention</strong>.
                    </li>

                    <li>
                      <strong>Healthcare Robot Data Analysis:</strong>
                      Developed a solution for <strong>multi-sensor data acquisition and analysis</strong> from a healthcare robot,
                      supporting monitoring, diagnostics, and data-driven insights.
                    </li>

                    <li>
                      <strong>Milestone‚ÄìPGMS Integration:</strong>
                      Led the full reproduction of the <strong>P-Guard Management System (PGMS)</strong> within the
                      <strong>Milestone XProtect Smart Client</strong>, including the implementation and integration of the
                      <strong>PGuard API and SDK</strong> for seamless control, monitoring, and interoperability.
                    </li>

                    <li>
                      <strong>PhD Supervision (INNOV-TMSDEVICE ‚Äì AI Development):</strong>
                      Supervised the <strong>artificial intelligence development</strong> of a Ph.D. thesis under the
                      <strong>MOBIDOC program</strong>, in collaboration with the <strong>Laboratoire de M√©canique de Sousse (LMS)</strong>,
                      entitled <em>‚ÄúContribution to the development of an innovative device for objective quantification, analysis and
                      prevention of musculoskeletal disorders (MSDs)‚Äù</em>, awarded <strong>Best National Doctoral Thesis of the Year 2024</strong>.
                    </li>

                    <li>
                      <strong>P-Guard Automation Tests (PAT):</strong>
                      Led the development of <strong>automated test suites</strong> using <strong>Python</strong>,
                      <strong>Robot Framework</strong>, and <strong>CI/CD tools</strong> to validate 4G connectivity,
                      WebRTC live video streaming, network traffic stress resilience, PGMS HMI compliance (unit tests vs. specifications),
                      and end-to-end robot automation workflows.
                    </li>
                  </ul>
                </div>
              </div>
              <button class="exp-toggle" type="button">Read more</button>
            </div>
          </div>
        </div>

      <!-- <div class="section-grid"> -->
        <div class="card">
          <div class="card-header-left">
              <img src="imgs/supcom.png" alt="supcom Logo" class="company-logo">
              <div>
                <div class="card-title">Computer Vision Researcher</div>
                <div class="card-subtitle">Sup‚ÄôCom ¬∑ Oct 2015 ‚Äì Dec 2018</div>

              </div>
          </div> 
          <div class="pill-list">
            <div class="pill-item">
              <p>
                Research on human posture recognition, including multispectral descriptors and edge detection methods.
              </p>
            </div>
          </div>         
        </div>

        <div class="card">
          <div class="card-header-left">
              <img src="imgs/dotit.jpeg" alt="dotit Logo" class="company-logo">
              <div>
                <div class="card-title">Computer Vision Developer</div>
                <div class="card-subtitle">Dot It Edition ¬∑ Aug 2014 ‚Äì Oct 2015</div>       
              </div>
          </div>
          <div class="pill-list">
            <div class="pill-item">
              <p>
                Developed computer vision components for various mobile projects, contributing to applications such
                as Yedess, GIREVE, AlMurafek and others.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- PUBLICATIONS -->
    <section id="publications">
  <div class="section-heading">
    <div>
      <div class="section-kicker">Scientific work</div>
      <h2 class="section-title">Publications & Reviewing</h2>
    </div>
  </div>

  <div class="section-grid">
    <div class="card">
      <div class="card-title">Selected journal & conference papers</div>

      <div class="pill-list">

        <div class="pill-item pub-pill">
          <strong>A novel multispectral corner detector and a new local descriptor: an application to human posture recognition</strong>

          <!-- <div class="pill-meta">
            ‚Äú‚Äù
          </div> -->

          <div class="pub-venue">Multimedia Tools and Applications ¬∑ 2023</div>

          <a class="pub-link" href="https://link.springer.com/article/10.1007/s11042-023-14788-1" target="_blank" rel="noopener noreferrer">
            Show publication <span aria-hidden="true">‚Üó</span>
          </a>

          <p class="pub-abstract" data-collapsed="true">
            Human posture recognition is an important task for intelligent systems specially those performing action recognition. In this paper, we propose a novel multispectral corner detector and a new HOG-based multispectral local descriptor. First, we select salient features which are extracted from an edge image obtained by picking the maximum eigenvalue of the jacobian matrix. Second, we extract for each feature point a local descriptor which combines both the Lab colour channels and depth information in a well-posed way using the Jacobian matrix. Last, we conduct a one-against-all learning strategy using both an incremental Covariance-guided One-Class Support Vector Machine (iCOSVM) and a Convolutional Neural Network (CNN). Experimental results show that we outperform the state-of-the-art methods whether our descriptor is combined with iCOSVM and with CNN.
          </p>

          <button class="exp-toggle" type="button">Read more</button>
        </div>

        <div class="pill-item pub-pill">
          <strong>Learning Human Postures using Lab-Depth HOG Descriptors</strong>

          <!-- <div class="pill-meta">
            ‚Äú.‚Äù
          </div> -->

          <div class="pub-venue">International Conference on Computational Collective Intelligence (ICCCI 2023) ¬∑ Sep 15, 2023</div>

          <a class="pub-link" href="https://link.springer.com/chapter/10.1007/978-3-031-41456-5_42" target="_blank" rel="noopener noreferrer">
            Show publication <span aria-hidden="true">‚Üó</span>
          </a>

          <p class="pub-abstract" data-collapsed="true">
            Human Posture Recognition is gaining increasing attention in the field of computer vision due to its promising applications in the areas of health care, human-computer interaction, and surveillance systems. This paper presents a novel method for human posture recognition by combining both color and depth images and feeding the resulting information into the vision transformer (ViT) model. We want to take advantage of integrating the Lab-D HOG descriptor [18] into the ViT architecture [8]. First, we compute the multispectral Lab-D edge detector by opting for the maximum eigenvalue of the multiplication of the jacobian matrix by its transpose. Second, we select the multispectral corner points by picking the minimum of the eigenvalues of the multispectral Harris matrix. Third, for each selected corner point, we compute a Lab-D HOG descriptor. Last, we feed the extracted Lab-D HOG descriptors into the transformer encoder/decoder by implementing two different strategies. Results show that we outperform state-of-the-art methods.
          </p>

          <button class="exp-toggle" type="button">Read more</button>
        </div>

        <div class="pill-item pub-pill">
          <strong>A Novel Multispectral Lab-depth based Edge Detector for Color Images with Occluded Objects</strong>
          <!-- <div class="pill-meta">
            ‚Äú‚Äù
          </div> -->

          <div class="pub-venue">VISAPP ¬∑ 2019</div>

          <a class="pub-link" href="https://www.scitepress.org/Papers/2019/73805/73805.pdf" target="_blank" rel="noopener noreferrer">
            Show publication <span aria-hidden="true">‚Üó</span>
          </a>

          <p class="pub-abstract" data-collapsed="true">
            This paper presents a new method for edge detection based on both Lab color and depth images. The principal challenge of multispectral edge detection consists of integrating different information into one meaningful result, without requiring empirical parameters. Our method combines the Lab color channels and depth information in a well-posed way using the Jacobian matrix. Unlike classical multi-spectral edge detection methods using depth information, our method does not use empirical parameters. Thus, it is quite straightforward and efficient. Experiments have been carried out on Middlebury stereo dataset and several selected challenging images. Experimental results show that the proposed method outperforms recent relevant state-of-the-art methods.
          </p>

          <button class="exp-toggle" type="button">Read more</button>
        </div>

      </div>
    </div>

    <div class="card">
      <div class="card-title">Program committee & reviewing</div>
      <p>I regularly act as a reviewer for international conferences in AI and intelligent environments.</p>
      <div class="pill-list" style="margin-top:8px;">
        <div class="pill-item">
        <div class="pill-top">
          <strong>ECAI</strong>

          <a class="pill-link-inline" href="https://ecai2025.org/program-committee/" target="_blank" rel="noopener noreferrer">
            Program committee ‚Üó
          </a>
        </div>

        <div class="pill-meta">European Conference on Artificial Intelligence (rank A) ¬∑ 2025</div>
      </div>
      </div>
    </div>
  </div>
</section>


    <!-- CONTACT -->
    <section id="contact">
      <div class="section-heading">
        <div>
          <div class="section-kicker">Get in touch</div>
          <h2 class="section-title">Contact</h2>
        </div>
      </div>

      <div class="contact-grid">
        <div class="card">
          <div class="card-title">Let‚Äôs talk</div>
          <p>
            I am open to collaborations, R&D opportunities, and discussions around AI for computer vision, semi-supervised
            learning, embedded deployment and intelligent video analytics.
          </p>
          <div class="contact-row">
            <div class="contact-tag">üìß mefteh.safa9@gmail.com</div>
            <div class="contact-tag">üìç Mons, Belgium</div>
            <div class="contact-tag">üîó LinkedIn ¬∑ linkedin.com/in/mefteh-safa</div>
            <div class="contact-tag">üíº GitHub ¬∑ github.com/meftehs</div>
          </div>
          <div class="contact-note">
            
          </div>
        </div>

        <div class="card">
          <div class="card-title">Quick summary</div>
          <p>
            Computer Vision and Edge AI specialist with a Ph.D. background, focusing on turning complex visual data into reliable, real-time intelligence. Experienced in building high-performance detection systems, improving learning with limited labels, and analyzing model behavior through explainability tools. Strong track record of deploying AI solutions on embedded platforms for robotics and smart video applications, from prototyping to production.      </p>
        </div>
      </div>
    </section>
  </main>

  <!-- FOOTER -->
  <footer>
    <div>¬© <span id="year"></span> Safa Mefteh. All rights reserved.</div>
    <div>Static site hosted on <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.</div>
  </footer>
</div>

<script>
  document.getElementById("year").textContent = new Date().getFullYear();
</script>
</body>
</html>
