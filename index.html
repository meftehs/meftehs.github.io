<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Dr Safa Mefteh, Ph.D., Eng. ‚Äì AI & Computer Vision Researcher & Developer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!-- <meta name="description" content="Portfolio of Dr Safa Mefteh, AI & Computer Vision Researcher & Developer." /> -->
  <meta name="description" content="Dr. Safa Mefteh ‚Äì AI & Computer Vision Researcher & Developer based in Belgium. Specialized in object detection, semi-supervised learning, explainability, and embedded AI.">
  <meta name="keywords" content="Safa Mefteh, AI Researcher, Computer Vision, Machine Learning, Belgium, Object Detection, Semi-Supervised Learning, NVIDIA Jetson">
  <meta name="author" content="Safa Mefteh">

  <!-- Google Font (optional) -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
  
  <link rel="icon" type="image/png" href="imgs/safa_71x64.png">

</head>

<script>
document.addEventListener("DOMContentLoaded", () => {
  const items = Array.from(document.querySelectorAll(".pill-item"));

  function needsToggle(el) {
    el.setAttribute("data-collapsed", "false");
    const full = el.scrollHeight;
    el.setAttribute("data-collapsed", "true");
    const clamped = el.getBoundingClientRect().height;
    return full > clamped + 2;
  }

  items.forEach(item => {
    const text = item.querySelector(".exp-text");
    const btn  = item.querySelector(".exp-toggle");
    if (!text || !btn) return;

    if (!needsToggle(text)) {
      btn.classList.add("is-hidden");
      text.setAttribute("data-collapsed", "false");
      return;
    }

    btn.addEventListener("click", () => {
      const collapsed = text.getAttribute("data-collapsed") === "true";

      items.forEach(other => {
        if (other === item) return;
        const ot = other.querySelector(".exp-text");
        const ob = other.querySelector(".exp-toggle");
        if (ot && ob && !ob.classList.contains("is-hidden")) {
          ot.setAttribute("data-collapsed", "true");
          ob.textContent = "Read more";
        }
      });

      text.setAttribute("data-collapsed", collapsed ? "false" : "true");
      btn.textContent = collapsed ? "Read less" : "Read more";
    });
  });
});
</script>


<body>
<div class="page">
  <!-- NAVBAR -->
  <header class="navbar">
    <div class="nav-inner">
      <div class="nav-brand">
        <div class="nav-avatar">S</div>
        <div>
          <div class="nav-title-main">Dr Safa Mefteh, Ph.D., Eng.</div>
          <div class="nav-title-sub">AI & Computer Vision Researcher & Developer</div>
        </div>
      </div>
      <nav class="nav-links">
        <a href="#about">About</a>
        <a href="#skills">Skills</a>
        <a href="#experience">Experience</a>
        <a href="#publications">Publications</a>
        <a href="#contact">Contact</a>
      </nav>
    </div>
  </header>

  <!-- HERO -->
  <main>
    <section class="hero">
      <div class="hero-left">
        <div class="hero-tag">
          <span class="hero-tag-dot"></span>
          <span>Applied AI ¬∑ Computer Vision ¬∑ Edge & Robotics</span>
        </div>

        <h1 class="hero-title">
          I'm Safa Mefteh, Ph.D.
        </h1>

        <p class="hero-subtitle">
          I‚Äôm an <strong>AI &amp; Computer Vision researcher</strong> with a passion for building 
          <strong>intelligent perception systems</strong> that operate reliably in real-world environments. 
          My work spans <strong>object detection</strong>, <strong>multimodal learning</strong>, 
          <strong>model explainability</strong>, and <strong>embedded deployment</strong>, 
          with a strong focus on translating advanced deep learning algorithms into 
          <strong>robust, real-time solutions on edge devices</strong>. 
          I thrive at the intersection of <strong>research and engineering</strong>, 
          where performance, efficiency, and interpretability must coexist.
        </p>
        <div class="hero-buttons">
          <a href="#experience" class="btn-primary">View my experience</a>
          <a href="#publications" class="btn-outline">Scientific publications</a>
        </div>

        <div class="hero-meta">
          <div class="hero-meta-item">
            <span class="hero-meta-dot"></span>
            <span>AI, Computer Vision & Machine Learning</span>
          </div>
          <div class="hero-meta-item">
            <span class="hero-meta-dot" style="background:#27ae60;"></span>
            <span>Object detection ¬∑ SSOD ¬∑ Explainability</span>
          </div>
          <div class="hero-meta-item">
            <span class="hero-meta-dot" style="background:#f2994a;"></span>
            <span>Edge AI ¬∑ Robotics ¬∑ Smart Video Analysis</span>
          </div>
        </div>
      </div>

      <div class="hero-right">
        <div class="profile-card">
          <div class="profile-header">
            <!-- <div class="profile-avatar" style="
                width:90px;
                height:90px;
                border-radius:50%;
                overflow:hidden;
                box-shadow:0 8px 18px rgba(47,128,237,0.25);">
              <img src="safa2.png" alt="Safa Mefteh" style="width:100%; height:100%; border-radius:50%; object-fit:cover;">
            </div> -->
            <div class="profile-avatar">
              <img src="imgs/safa.png" alt="Safa Mefteh">
            </div>
            <div>
              <div class="profile-name">Dr Safa Mefteh, Ph.D., Eng.</div>
              <div class="profile-role">AI & Computer Vision Researcher & Developer</div>
              <div class="profile-pill">
                Currently ¬∑ Computer Vision & ML researcher & developer, Multitel
              </div>
            </div>
          </div>

          <div class="profile-tags">
            <span class="chip">Deep Learning</span>
            <span class="chip">Computer Vision</span>
            <span class="chip">Edge AI</span>
            <span class="chip">Research & Development (R&D)</span>
            <span class="chip">C++ ¬∑ Python ¬∑ PyTorch ¬∑ OpenCV</span>
          </div>

          <div class="profile-footer">
            <span>üéì <strong>Ph.D.</strong><br/>Computer Vision & Deep Learning</span>
            <span>üõ† <strong>Industry</strong><br/>Robotics & Intelligent Video Analytics</span>
          </div>
        </div>
      </div>
    </section>

    <!-- ABOUT & EDUCATION -->
    <section id="about">
      <div class="section-heading">
        <div>
          <div class="section-kicker">Profile</div>
          <h2 class="section-title">About & Education</h2>
        </div>
      </div>

      <div class="section-grid">
        <div class="card">
          <div class="card-title">About me</div>
          <p>
            I work at the intersection of <strong>advanced algorithms</strong> (computer vision, deep learning, multimodal learning, explainability) 
            and <strong>real-world systems</strong> where latency, reliability, and hardware efficiency are critical. 
            I design, train, and deploy <strong>high-performance perception models</strong> with a strong focus on 
            <strong>Edge AI optimization</strong>, transforming complex visual data into robust, real-time intelligence on embedded platforms.
          </p>

          <div class="badge-row" style="margin-top:10px;">
            <span class="badge">Computer Vision & Deep Learning</span>
            <span class="badge">Edge AI ¬∑ NVIDIA Jetson</span>
            <span class="badge">Explainability ¬∑ Multimodal ML</span>
            <span class="badge">Efficient model deployment</span>
            <span class="badge">Research & Engineering synergy</span>
          </div>
        </div>

        <div class="card">
          <div class="card-title">Education</div>
          <div class="timeline">
            <div class="timeline-item">
              <div class="timeline-marker"></div>
              <div>
                <div class="timeline-title">Ph.D. in Computer Vision & Deep Learning</div>
                <div class="timeline-meta">Higher School of Communication of Tunis (Sup‚ÄôCom), University of Carthage </div>
              </div>
            </div>
            <div class="timeline-item">
              <div class="timeline-marker"></div>
              <div>
                <div class="timeline-title">Engineer‚Äôs Degree in Computer Science</div>
                <div class="timeline-meta">National School of Computer Science, Tunisia </div>
              </div>
            </div>
            <div class="timeline-item">
              <div class="timeline-marker"></div>
              <div>
                <div class="timeline-title">Preparatory Diploma in Mathematics & Physics</div>
                <div class="timeline-meta">Preparatory Institute for Studies of Engineers of Monastir </div>
              </div>
            </div>
            <div class="timeline-item">
              <div class="timeline-marker"></div>
              <div>
                <div class="timeline-title">Bachelor‚Äôs Degree in Mathematics</div>
                <div class="timeline-meta">Hammam Sousse 2 High School, Tunisia </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- SKILLS -->
    <section id="skills">
      <div class="section-heading">
        <div>
          <div class="section-kicker">What I work with</div>
          <h2 class="section-title">Skills</h2>
        </div>
        <div class="section-note">
          A mix of research skills, engineering practices and deployment experience.
        </div>
      </div>

      <div class="section-grid">
        <!-- <div class="card"> -->
          <div class="card">
            <div class="card-title">Technical skills</div>

            <div class="badge-row">
              <span class="badge">Computer Vision</span>
              <span class="badge">Deep Learning</span>
              <span class="badge">Multimodal Perception</span>
              <span class="badge">Semi-Supervised Object Detection</span>
              <span class="badge">Model Explainability </span>
              <span class="badge">Detection ¬∑ Segmentation ¬∑ Tracking</span>
              <span class="badge">Pseudo-Labeling & Multi-View Learning</span>
              <span class="badge">Feature Engineering</span>
            </div>

            <div class="badge-row" style="margin-top:10px;">
              <span class="badge">Python ¬∑ C++</span>
              <span class="badge">PyTorch</span>
              <span class="badge">TensorRT</span>
              <span class="badge">DeepStream</span>
              <span class="badge">NVIDIA Jetson (Nano ¬∑ Xavier ¬∑ Orin)</span>
              <span class="badge">GPU Optimization</span>
              <span class="badge">Docker ¬∑ Git</span>
              <span class="badge">ROS</span>
            </div>
          </div>

          <div class="card">
            <div class="card-title">Soft skills & languages</div>
            <p>
              I appreciate clear teamwork, structured projects, and a collaborative way of working.
            </p>

            <div class="badge-row" style="margin-top:10px;">
              <span class="badge">Team leadership</span>
              <span class="badge">Mentoring & knowledge sharing</span>
              <span class="badge">Project ownership</span>
              <span class="badge">Cross-functional collaboration</span>
            </div>

            <div class="badge-row" style="margin-top:14px;">
              <span class="badge">French ¬∑ Fluent</span>
              <span class="badge">Arabic ¬∑ Native</span>
              <span class="badge">English ¬∑ Professional working proficiency</span>
            </div>
          <!-- </div> -->
    </section>

    <!-- EXPERIENCE -->
    <section id="experience">
      <div class="section-heading">
        <div>
          <div class="section-kicker">Career path</div>
          <h2 class="section-title">Experience</h2>
        </div>
      </div>

      <div class="section-grid">
        <div class="card">
          <div class="card-header-left">
            <img src="imgs/multitel.png" alt="Multitel Logo" class="company-logo">
            <div>
              <div class="card-title">Computer Vision &amp; ML Researcher &amp; Developer</div>
              <div class="card-subtitle">Multitel ¬∑ Jun 2023 ‚Äì Present ¬∑ Mons, Belgium</div>
            </div>
          </div>

          <div class="pill-list">
            <!-- Project 1: SSOD -->
            <div class="pill-item">
              <strong>Advanced Semi-Supervised Object Detection (SSOD) for Real-Time Edge Deployment</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                <p>
                  Large-scale, high-quality annotations are often <strong>scarce</strong> and <strong>costly</strong>. 
                  Semi-supervised learning leverages abundant <strong>unlabeled data</strong> to preserve strong detection performance 
                  while reducing annotation dependency. A key challenge is minimizing the <strong>distribution gap</strong> between 
                  labeled and unlabeled data‚Äîespecially across diverse and dynamic contexts. Another critical issue is generating 
                  <strong>reliable pseudo-labels</strong>, since early errors can propagate and degrade overall performance.
                </p>

                <p>
                  To address these challenges, we designed and deployed a <strong>real-time SSOD framework</strong> for one-stage 
                  RGB video analysis on <strong>edge devices</strong>. Our approach integrates foundation models 
                  (<strong>Grounding-DINO</strong> and <strong>YOLO-World</strong>) to improve object coverage and semantic robustness, 
                  achieving a <strong>+3 mAP</strong> improvement over state-of-the-art baselines. In addition, we introduced a 
                  redesigned pseudo-labeling strategy combining <strong>multi-view consistency</strong> with 
                  <strong>fusion-based optimization</strong>, reducing label noise and narrowing the labeled‚Äìunlabeled domain gap, 
                  yielding a further <strong>+4 mAP</strong> gain.
                </p>

                <p>
                  The framework was validated across <strong>multiple domains</strong> and optimized for <strong>real-time inference</strong> 
                  on <strong>NVIDIA Jetson</strong> platforms, demonstrating both <strong>robustness</strong> and 
                  <strong>deployment-level efficiency</strong>.
                </p>
                  
              </div>
                
              </div>
              <button class="exp-toggle" type="button">Read more</button>
            </div>
            <!-- Project 2: Interpretation Toolbox -->
            <div class="pill-item">
              <strong>Interpretation Toolbox for Object Detection</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                  <p>
                    <strong>Mean Average Precision (mAP)</strong> is the standard metric for evaluating object detection models, yet it provides only
                    a global performance score and offers limited insight into <strong>why</strong> a model succeeds or fails. A degradation in mAP may
                    originate from diverse error sources, such as <strong>misclassification</strong>, <strong>inaccurate localization</strong>,
                    <strong>missed detections</strong>, <strong>background false positives</strong>, or <strong>duplicated predictions</strong>.
                  </p>
                  Moreover, factors like <strong>object scale</strong>, <strong>distance to the camera</strong>, and <strong>visual conditions</strong>
                  can significantly affect detection quality, making a deeper and more structured analysis essential.
                <!-- </p> -->

                <p>
                  To address these limitations, we developed an <strong>interpretation toolbox</strong> for object detection that enables a fine-grained,
                  diagnostic evaluation of model behavior. The toolbox automatically categorizes detection errors‚Äîincluding
                  <strong>classification errors</strong>, <strong>localization errors</strong>, <strong>combined classification‚Äìlocalization errors</strong>,
                  <strong>missed objects</strong>, <strong>background false positives</strong>, and <strong>duplicate detections</strong>‚Äîand explicitly links
                  each error type to its contribution to <strong>mAP degradation</strong>. This allows practitioners to move beyond a single aggregate metric
                  and systematically identify the dominant sources of performance loss.
                </p>

                <p>
                  The toolbox decomposes mAP into interpretable error components and
                  quantifies the upper-bound performance achievable under perfect error correction. Detection results are visualized through an
                  <strong>interactive interface</strong> where objects are grouped by error type, enabling rapid identification of systematic failure modes
                  (e.g., poor performance on <strong>small</strong> or <strong>distant</strong> objects). To further uncover patterns across errors, detected
                  objects are clustered by applying a visual feature extraction, followed by a dimensionality
                  reduction for intuitive 2D visualization of error clusters.
                </p>

                <p>
                  In addition, the toolbox integrates <strong>pixel-level</strong>, perturbation-based explainability by generating
                  <strong>saliency maps</strong> that highlight the image regions most influential for each detection. This enables inspection of the model‚Äôs
                  internal decision process, revealing whether critical semantic cues‚Äîsuch as <strong>wheels</strong> in daylight scenes or
                  <strong>headlights</strong> at night for vehicle detection‚Äîare effectively leveraged. When such cues are absent or ignored, the toolbox helps
                  explain persistent misdetections or misclassifications.
                </p>

                <p>
                  Overall, this interpretation toolbox provides a comprehensive <strong>visual</strong> and <strong>quantitative</strong> analysis framework for
                  object detection models, improving <strong>trust</strong>, <strong>transparency</strong>, and <strong>diagnostic accuracy</strong>. It supports
                  informed model debugging, targeted data augmentation, and architecture refinement, making it particularly valuable for evaluating and
                  deploying object detection systems in <strong>real-world</strong> and <strong>safety-critical</strong> applications.
                </p>
              </div>
              </div>
              <button class="exp-toggle" type="button">Read more</button>
              <!-- <a class="pill-link" href="interpretation-toolbox.html">
                <span class="pill-icon">üìñ</span>
                Read more
              </a> -->
            </div>
            <!-- Project 3: VMS Integration -->
            <div class="pill-item">
              <strong>Edge-to-VMS Integration for Real-Time Video Analytics</strong>
              <div class="pill-meta">
                <div class="exp-text" data-collapsed="true">
                Integrated embedded detection models into a Video Management System (VMS) by connecting
                <strong>NVIDIA Jetson Orin</strong> inference pipelines to centralized video servers.
                Implemented custom bridging functions inspired by DeepStream SDK VPS‚ÄìMilestone workflows,
                enabling event streaming, metadata forwarding, and simulation of licensed VMS analytics features
                for seamless operational deployment.
              </div>
              </div>
              <button class="exp-toggle" type="button">Read more</button>
            </div>

          </div>
        </div>

        <div class="card">
          <div class="card-header card-header--with-project">
            <div class="card-header card-header-left">
              <img src="imgs/enova.png" alt="Enova Robotics Logo" class="company-logo">
              <div>
                <div class="card-title">
                  Senior ML &amp; Computer Vision R&amp;D Engineer ¬∑ AI Team Lead (P-Guard Robot)
                </div>
                <div class="card-subtitle">Enova Robotics ¬∑ Jan 2019 ‚Äì May 2023 ¬∑ Tunisia</div>
              </div>
            </div>

            <img src="imgs/pguard.jpeg" alt="P-Guard Robot" class="project-logo">
          </div>

          <div class="pill-list">
            <div class="pill-item">
              <strong>Autonomous Charging Station</strong>
              <div class="pill-meta">
                Implemented cosses detection and tracking to automatically connect the robot to a charging station,
                ensuring maximum autonomy using computer vision control on an X‚ÄìY table.
              </div>
            </div>

            <div class="pill-item">
              <strong>Intrusion Detection (Day &amp; Night)</strong>
              <div class="pill-meta">
                Designed and deployed a smart intrusion detection solution combining optical and thermal cameras,
                running on NVIDIA Jetson Nano via DeepStream SDK, integrated with Milestone XProtect VMS for alarm generation.
              </div>
            </div>

            <div class="pill-item">
              <strong>Obstacle Detection &amp; Fence Anomaly Detection</strong>
              <div class="pill-meta">
                Processed 3D LiDAR data for obstacle detection and classification; developed a solution to classify broken vs.
                normal fences using Pix2Pix GAN, segmentation and CNN-based classification.
              </div>
            </div>

            <div class="pill-item">
              <strong>Project Management &amp; Automation</strong>
              <div class="pill-meta">
                Led several projects including mission scheduling for P-Guard, healthcare robot data analysis, Milestone‚ÄìPGMS
                integration, supervision of a PhD (INNOV-TMSDEVICE) awarded Best National Doctoral Thesis 2024, and
                automation tests (PAT) using Python, Robot Framework and CI/CD tools.
              </div>
            </div>
          </div>
        </div>

      <!-- <div class="section-grid"> -->
        <div class="card">
          <div class="card-header-left">
              <img src="imgs/supcom.png" alt="supcom Logo" class="company-logo">
              <div>
                <div class="card-title">Computer Vision Researcher</div>
                <div class="card-subtitle">Sup‚ÄôCom ¬∑ Oct 2015 ‚Äì Dec 2018</div>
                <p>
                    Research on human posture recognition, including multispectral descriptors and edge detection methods.
                </p>
              </div>
          </div>          
        </div>

        <div class="card">
          <div class="card-header-left">
              <img src="imgs/dotit.jpeg" alt="dotit Logo" class="company-logo">
              <div>
                <div class="card-title">Computer Vision Developer</div>
                <div class="card-subtitle">Dot It Edition ¬∑ Aug 2014 ‚Äì Oct 2015</div>
                  <p>
                    Developed computer vision components for various web and software projects, contributing to applications such
                    as Yedess, GIREVE, AlMurafek and others.
                  </p>
              </div>
          </div>
        </div>
      </div>
    </section>

    <!-- PUBLICATIONS -->
    <section id="publications">
      <div class="section-heading">
        <div>
          <div class="section-kicker">Scientific work</div>
          <h2 class="section-title">Publications & Reviewing</h2>
        </div>
      </div>

      <div class="section-grid">
        <div class="card">
          <div class="card-title">Selected journal & conference papers</div>
          <div class="pill-list">
            <div class="pill-item">
              <strong>Multispectral corner detector & local descriptor</strong>
              <div class="pill-meta">
                ‚ÄúA novel multispectral corner detector and a new local descriptor: an application to human posture recognition.‚Äù
                Multimedia Tools and Applications, 2023.
              </div>
            </div>
            <div class="pill-item">
              <strong>Lab-Depth HOG Descriptors for human postures</strong>
              <div class="pill-meta">
                ‚ÄúLearning Human Postures using Lab-Depth HOG Descriptors.‚Äù ICCCI 2023, Springer LNAI.
              </div>
            </div>
            <div class="pill-item">
              <strong>Multispectral edge detector for occluded color images</strong>
              <div class="pill-meta">
                ‚ÄúA Novel Multispectral Lab-depth based Edge Detector for Color Images with Occluded Objects.‚Äù
                VISAPP 2019.
              </div>
            </div>
            <div class="pill-item">
              <strong>Vision transformer for posture recognition</strong>
              <div class="pill-meta">
                ‚ÄúHuman Posture Recognition based on a novel vision transformer using a multispectral Lab-D HoG Descriptor.‚Äù
                AFJASSST, 2022.
              </div>
            </div>
          </div>
        </div>

        <div class="card">
          <div class="card-title">Program committee & reviewing</div>
          <p>
            I regularly act as a reviewer for international conferences in AI and intelligent environments.
          </p>
          <div class="pill-list" style="margin-top:8px;">
            <div class="pill-item">
              <a href="https://ecai2025.org/program-committee/" target="_blank" rel="noopener noreferrer">
                <strong>ECAI</strong>
              </a>
              <div class="pill-meta">
                European Conference on Artificial Intelligence (A ranking), 2025.
              </div>
            </div>
          </div>
        </div>
    </section>

    <!-- CONTACT -->
    <section id="contact">
      <div class="section-heading">
        <div>
          <div class="section-kicker">Get in touch</div>
          <h2 class="section-title">Contact</h2>
        </div>
      </div>

      <div class="contact-grid">
        <div class="card">
          <div class="card-title">Let‚Äôs talk</div>
          <p>
            I am open to collaborations, R&D opportunities, and discussions around AI for computer vision, semi-supervised
            learning, embedded deployment and intelligent video analytics.
          </p>
          <div class="contact-row">
            <div class="contact-tag">üìß mefteh.safa9@gmail.com</div>
            <div class="contact-tag">üìç Mons, Belgium</div>
            <div class="contact-tag">üîó LinkedIn ¬∑ linkedin.com/in/mefteh-safa</div>
            <div class="contact-tag">üíº GitHub ¬∑ github.com/meftehs</div>
          </div>
          <div class="contact-note">
            Feel free to reach out in English or French.
          </div>
        </div>

        <div class="card">
          <div class="card-title">Quick summary</div>
          <p>
            Ph.D.-level AI & Computer Vision profile with strong industrial experience in robotics and embedded systems.
            I like projects where research and engineering are both required to reach production quality.
          </p>
        </div>
      </div>
    </section>
  </main>

  <!-- FOOTER -->
  <footer>
    <div>¬© <span id="year"></span> Safa Mefteh. All rights reserved.</div>
    <div>Static site hosted on <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.</div>
  </footer>
</div>

<script>
  document.getElementById("year").textContent = new Date().getFullYear();
</script>
</body>
</html>
